# Lecture Transcriber Configuration File
# This file contains default settings for the lecture transcriber system
#
# Design Principles Applied:
# - KISS: Simple YAML configuration over complex setup procedures
# - DRY: Unified Hugging Face ecosystem eliminates duplicate model configurations
# - YAGNI: Only essential configuration options, no over-engineering
# - Single Responsibility: Each section handles one aspect of the system

# Virtual Environment Settings
environment:
  venv_path: "./lecture_transcriber_env"
  model_cache_path: "./lecture_transcriber_env/model_cache"
  
# Whisper Model Configuration
whisper:
  # Available models with their Hugging Face identifiers
  # Default model for balanced performance
  default_model: "openai/whisper-medium"
  
  # GPU acceleration settings
  gpu_acceleration:
    enabled: true
    torch_dtype: "float16"  # Use half precision on GPU for speed
    device_map: "auto"      # Automatic device selection
    use_flash_attention: true  # Faster attention computation
  
  # Model-specific settings
  models:
    "openai/whisper-tiny":
      memory_requirement_mb: 500
      recommended_for: "Быстрое тестирование, системы с ограниченными ресурсами"
      language_support: "Многоязычный, базовое качество"
      
    "openai/whisper-base":
      memory_requirement_mb: 1000
      recommended_for: "Базовое качество, быстрый отклик"
      language_support: "Многоязычный, приемлемое качество"
      
    "openai/whisper-medium":
      memory_requirement_mb: 2000
      recommended_for: "Сбалансированная скорость и качество"
      language_support: "Многоязычный, хорошее качество"
      
    "openai/whisper-large":
      memory_requirement_mb: 6000
      recommended_for: "Высокое качество, требует достаточно ресурсов"
      language_support: "Многоязычный, отличное качество"
      
    "openai/whisper-large-v3":
      memory_requirement_mb: 8000
      recommended_for: "Высочайшее качество, требует мощное оборудование"
      language_support: "Многоязычный, лучшее качество"
      
    "openai/whisper-large-v3-turbo":
      memory_requirement_mb: 6000
      recommended_for: "Быстрая обработка с высоким качеством"
      language_support: "Многоязычный, оптимизированная скорость"
      
    "antony66/whisper-large-v3-russian":
      memory_requirement_mb: 8000
      recommended_for: "Специально для русского языка, максимальная точность"
      language_support: "Русский язык, специализированная модель"

# Audio Processing Settings
audio:
  # Supported input formats
  supported_formats: ["mp4", "mkv", "webm", "avi", "mov"]
  
  # Output format for extracted audio
  output_format: "wav"
  
  # Audio quality settings
  sample_rate: 16000  # Hz, optimal for Whisper
  channels: 1  # Mono audio
  bitrate: "128k"

# Voice Activity Detection (VAD) Settings
vad:
  # VAD confidence threshold (0.0-1.0)
  # Higher values = more conservative speech detection
  threshold: 0.5
  
  # Minimum duration for speech segments (seconds)
  # Segments shorter than this will be filtered out
  min_speech_duration: 0.25
  
  # Minimum duration for silence segments (seconds)
  # Silence shorter than this will be merged with adjacent speech
  min_silence_duration: 0.1
  
  # Maximum gap between speech segments to merge them (seconds)
  max_merge_gap: 0.5
  
  # Enable VAD processing (set to false to disable)
  enabled: true
  
  # Sample rate for VAD processing (16000 Hz is optimal for Silero VAD)
  sample_rate: 16000

# Hallucination Filter Settings
hallucination_filter:
  # Compression ratio threshold (higher = more repetitive)
  # Segments with compression ratio above this will be filtered
  # Default: 1.8 (Whisper default is 2.4, we use more aggressive filtering)
  compression_ratio_threshold: 1.8
  
  # Log probability threshold (lower = less confident)
  # Segments with average log probability below this will be filtered
  # Default: -0.8 (Whisper default is -1.0, we use more aggressive filtering)
  logprob_threshold: -0.8
  
  # Enable/disable specific filter types
  enable_blacklist_filter: true    # Filter known hallucination phrases
  enable_pattern_filter: true      # Filter regex patterns (URLs, social media, etc.)
  enable_compression_filter: true  # Filter high compression ratio segments
  enable_logprob_filter: true      # Filter low confidence segments
  
  # Custom blacklist phrases (in addition to built-in defaults)
  # Add domain-specific phrases that should be filtered
  custom_blacklist:
    # Add your custom phrases here
    # - "custom phrase 1"
    # - "custom phrase 2"

# Text Preprocessing Settings
preprocessing:
  filler_words:
    - "эм"
    - "ээ"
    - "ну"
    - "типа"
    - "короче"
    - "как бы"
    - "в общем"
    - "значит"
    - "вот"
    - "это самое"
  
  # Pause markers to clean up
  pause_markers:
    - "..."
    - "[пауза]"
    - "[тишина]"
    - "***"
  
  # Repetition handling
  max_consecutive_repeats: 1
  
  # Cleaning intensity (1-3, where 3 is most aggressive)
  cleaning_intensity: 2

llm:
  # Provider: "huggingface" or "ollama"
  provider: "huggingface"
  
  # Default model for text generation and summarization
  default_model: "Qwen/Qwen2.5-1.5B-Instruct"

  huggingface:
    # Primary model for segment merging and text generation
    model_name: "Qwen/Qwen2.5-1.5B-Instruct"
    max_length: 2048
    temperature: 0.7
    top_p: 0.9
    do_sample: true
    pad_token_id: 50256
    
    # Model-specific parameters with GPU support
    torch_dtype: "auto"  # Automatic precision selection (float16 on GPU, float32 on CPU)
    device_map: "auto"   # Automatic device placement (GPU if available, CPU fallback)
    trust_remote_code: false
    
    # GPU optimization settings
    gpu_optimization:
      use_flash_attention: true   # Faster attention on compatible GPUs
      load_in_8bit: false        # 8-bit quantization for memory efficiency
      load_in_4bit: false        # 4-bit quantization for maximum memory savings
      use_cache: true            # Enable KV cache for faster generation
    
    # Generation parameters for different tasks
    segment_merging:
      max_new_tokens: 512
      temperature: 0.3
      top_p: 0.8
      repetition_penalty: 1.1
      
    summarization:
      max_new_tokens: 256
      temperature: 0.5
      top_p: 0.9
      repetition_penalty: 1.2
      
    technical_content_identification:
      max_new_tokens: 128
      temperature: 0.2
      top_p: 0.7
      repetition_penalty: 1.0
    
  # Ollama settings (alternative local deployment)
  ollama:
    base_url: "http://localhost:11434"
    model_name: "phi3:mini"
    timeout: 60
    
    # Ollama-specific parameters
    temperature: 0.7
    top_p: 0.9
    repeat_penalty: 1.1

# Mathematical Formula Processing
formulas:
  greek_letters:
    "альфа": "α"
    "бета": "β"
    "гамма": "γ"
    "дельта": "δ"
    "эпсилон": "ε"
    "дзета": "ζ"
    "эта": "η"
    "тета": "θ"
    "йота": "ι"
    "каппа": "κ"
    "лямбда": "λ"
    "мю": "μ"
    "ню": "ν"
    "кси": "ξ"
    "омикрон": "ο"
    "пи": "π"
    "ро": "ρ"
    "сигма": "σ"
    "тау": "τ"
    "ипсилон": "υ"
    "фи": "φ"
    "хи": "χ"
    "пси": "ψ"
    "омега": "ω"

  operations:
    "плюс": "+"
    "минус": "-"
    "умножить": "×"
    "разделить": "÷"
    "равно": "="
    "больше": ">"
    "меньше": "<"
    "больше или равно": "≥"
    "меньше или равно": "≤"
    "не равно": "≠"
    "приблизительно": "≈"
    "бесконечность": "∞"
    "интеграл": "∫"
    "сумма": "∑"
    "произведение": "∏"
    "корень": "√"
    "степень": "^"
  
  # Confidence threshold for formula conversion (0.0-1.0)
  confidence_threshold: 0.8

# Output Generation Settings
output:
  formats:
    markdown: true
    json_metadata: true

  markdown:
    include_timestamps: true
    timestamp_format: "[{start_time:.1f}s - {end_time:.1f}s]"
    include_confidence_scores: false

  metadata:
    include_processing_stats: true
    include_model_info: true
    include_system_info: true

  review:
    create_review_section: true
    confidence_threshold: 0.7  # Flag content below this confidence
    max_review_items: 50

# System Resource Management
resources:
  # Device selection for ML models
  device: "auto"  # Options: "auto", "cpu", "cuda", "mps" (Apple Silicon)
  
  # GPU-specific settings
  gpu:
    enabled: true
    memory_fraction: 0.8  # Use 80% of GPU memory
    allow_growth: true    # Gradually allocate GPU memory
    fallback_to_cpu: true # Fallback if GPU unavailable
  
  # Memory limits (in MB)
  max_memory_usage: 8192
  
  max_video_duration_minutes: 180  # 3 hours
  max_file_size_mb: 2048  # 2 GB
  
  # Parallel processing
  max_workers: 4

  cleanup_temp_files: true
  temp_dir: "./temp"

# Logging Configuration
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR, CRITICAL
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "logs/lecture_transcriber.log"
  max_file_size_mb: 10
  backup_count: 5

development:
  debug_mode: false
  save_intermediate_files: false
  profile_performance: false

testing:
  property_test_iterations: 100
  test_data_dir: "./tests/data"
  mock_llm_responses: false
